### 初识Hadoop
* hadoop2.x的特性
    ```
    特性 | 安全认证 旧的配置项 新的配置项 旧的MR-API 新的MR-API MR1的运行环境 MR2的运行环境 HDFS联邦管理 HDFS的高可用
    ============================================================================================================
    2.x  | √        ×           √          √           √           ×              √             √            √   
    ```
### mapreduce
* 过程
    ```
    Input       =>      Map        =>      Shuffle         =>          Reduce        =>      Output
    ------------------------------------------------------------------------------------------------
    file等    (k11,v11)        (k21,v211)          (k21,[v211,v212,..])          (k21,v31)    file等
              (k12,v12)        (k21,v212)         (k22,[v221,..])               (k22,v32)
              ...              (k22,v221)          ...                           ...
                                ...
    ```
1. 输入
    * 分片（split）
        * MR将输入的数据划分成等长的小数据块，hadoop为每个分片创建一个map任务
        * 虽然分片被切的越细，负载均衡会更好。但是也要考虑最小化寻址开销，数据块大则块数据传输时间远大于块的寻址时间MR集群中的map数量，分片大小，分片/map数量要做权衡。
            同时，分片的大小也不能太大，太大就会导致一个分片跨越多个数据块。而**通常情况下这些多个数据块会存储在不同的
            主机上**。（跟数据块副本落在不同的主机上类似，就是为了减少数据丢失的可能）
        * **合理的分片大小就是将分片大小设置为hdfs的块大小**，而一个hdfs文件若小于hdfs的块大小，
            则其逻辑占据一个块大小，实际物理上占据的就是其真实大小。这也是为什么默认情况下，一个hdfs文件
            就是一个分片（这个hdfs文件大小小于一个hdfs的块大小）
    * 数据本地化优化
        * 原因：
            由于map执行在本机器上，而map需要的数据文件存储在其他主机上，则这个时候就要消耗网络带宽与传输时间。
        * 措施：
            * 故尽可能让执行MR的程序与其所需要的数据存在于同一物理主机上。
         
2. map
    * map读取分片
        * 分片由InputFormat产生，并由其分割成记录
        ```
        // Mapper的run方法
        void run(ctx) {
            setup(ctx);
            while (ctx.nextKeyValue()) { // 委托调用recordReader.nextKeyValue()
                map(ctx.getCurrentKey(), ctx.getCurrentValue(), ctx);
            }
            cleanup(ctx);
        }
        ```
    * map输出的中间结果
        * 输出位置：执行这个map任务主机的本地磁盘上
            * 输出到本地磁盘的原因：map产生的只是中间结果，直接结果是供reduce使用产生最终结果的。
                一旦job完成，map的中间结果就会被删除。故不需要小题大做存储到hdfs上
    * 当传输map的中间结果之前，map就失败了
        * 解决方案：
            * 在MR集群的另一个主机上启动map任务来构建中间结果。（当然选定新的map任务节点肯定也
                优先考虑有map所需数据副本的主机上执行map任务）
    
                
3. shuffle
    1. map函数通过调用content.write方法向环形内存缓冲区写入<k,v>
    2. 当缓冲区达到阈值，启动溢写线程同时仍可继续往剩余的缓冲区继续写，当缓冲区满的时候阻塞map直到溢写完成。
        * 默认缓冲区大小为100MB，阈值为80%
    3. 在溢写线程中，首先新建一个溢写文件
    4. 在溢写之前后台线程首先在缓冲区这80%的数据进行按reduce的数量进行分区，然后按每个分区内部进行快速排序然后分组
        （即同key的val被规约）。
    5. 若有combiner，此时则对数据进行操作，使得map的输出更加紧凑
    6. 然后将数据写入溢写文件中
    7. 当完成map任务之前可能有多个溢写文件，将对溢写文件再进行归并成一个文件
4. combine(可有可无)
    * 数据处理
        ```
        (k21, [v211,v212,v213,..])     combine      (k21, v212)
        (k22, [v221,v222,v223,..])   ===========>   (k22, v223)
        ...                                          ...
        ```
    * 将shuffle后的map输出到磁盘之前先做一步处理类似reduce，目的是为了减少网络数据传输量。
5. reduce
    * reduce的输入通常来自一个或多个map的输出，通常将map的中间输出在本地进行多路归并，最终reduce函数读入有序的输入
        **一般为了可靠性将最终结果放在hdfs存储**
        * 结果的存储执行hdfs副本存储策略：
            1. 存储在本地节点上 
            2. 存储在同一机架上的其他节点上 
            3. 存储在其他机架上任意节点上
        
6. 输出

7. 设置计数器
    * mapred中计数器是全局的，在运行期由tasktracker统计，并最终由jibtracker回收聚合。
    1. 静态计数器
        * 由定义枚举类型，枚举类型就是计数器的分组，枚举的类别就是该计数器的名字
        * 在map/reduce中都可用：ctx.getCounter(MyEnum.MY_COUNTER).increment(n);
    2. 动态计数器
        * ctx.getCounter(String group, String counter)
#### mapreduce的运行机制
* MR1的作业运行机制
    1. cli从jobtracker中获取新的job ID
    2. 检查输出目录以及做数据分片，然后将job jar包以及一系列相关的数据放到共享文件系统中
    3. 完成前期准备工作，向jobtracker提交job
    4. jobtracker将job放入调度队列中，等job被调度时，初始化该job对象，为其创建各种追踪信息。
    5. jobtracker从共享文件系统中读取该job的输入分片，为每个分片创建一个map任务，而reduce的任务数由配置参数指定。
    6. 满足本地优化原则的tasktrack从共享文件系统中读取分片以及所需数据，并在任务槽上启动map子进程
    7. 而reduce任务的启动可以不满足本地优化原则，期间map/reduce的人去进度由tasktracker通过心跳机制通报jobtracker，直到任务完成
* MR2-YARN的作业运行机制
    1. cli从资源管理器resource manager中获取最新的job ID
    2. 检查输出目录以及做数据分片，然后将job jar包以及一系列相关的数据放到共享文件系统中 
    3. 完成前期准备工作，向资源管理器提交job
    4. 资源管理器将job放入调度队列中，等job被调度时，为该job在某节点管理器上分配一个容器运行app master进程
    5. 该进程负责job任务的各项指标如进度的追踪，任务失败，重试等等，而资源管理器只负责job的调度与资源容器的分配 
    6. app master从共享文件系统中读取数据分片，为每一个分片分配一个map任务并向资源管理器申请容器启动map任务以及后面的reduce任务。
        * 注意MR1中的任务槽内存大小是固定的，而MR2中的容器分配的大小是弹性的大小是n*min <= max
    7. 在资源管理器分配容器后，app master就通知节点管理器上在目标容器中启动相应的map/reduce任务（新的进程），任务向每个一定时间向app
        master汇报任务进度，直至任务结束
* MR任务的进度
    * map任务的进度
        * 由输入数据百分比决定
    * reduce任务进度
        * 其进度由三部分组成
            1. 从所有map任务节点复制中间结果的复制阶段（占1/3）
            2. 对复制数据的排序阶段（占1/3）
            3. 对数据的处理（占1/3）
        * 比如reduce已经处理了数据50%则reduce任务的进度为1/3 + 1/3 + 1/3 * 1/2 = 5/6
* MR1任务执行失败的处理
    1. 任务失败
        * 比如用户代码抛出异常，子jvm进程异常中断或退出等等，tasktracker通过一段时间没有接受到
            任务子进程的反馈就将该任务标记为失败，并重启多次任务重试，当全部失败后jobtracker将吧这个job
            标记失败
    2. task失败
        * 当jobtracker一段时间没有接受到来自tasktracker的心跳反馈，就可以认为这个tasktracker gg了，
            接下来jobtracker就会对跑在这个tasktracker上的任务进行重新调度
    3. jobtracker失败                                   
        * 属于单点失效问题，只等重启jobtracker进行job的重新提交  
* MR2任务执行失败的处理
    1. 任务失败
        * 比如用户代码抛出异常，jvm进程异常中断或退出等等，tasktracker通过一段时间没有接受到
            任务子进程的反馈就将该任务标记为失败，并重启多次任务重试，当全部失败后jobtracker将吧这个job
            标记失败
    2. App master运行失败
        * 当资源管理器通过心跳机制发现app master挂了，则重新分配一个容器重启app master接管job
        * cli通过轮训app master来获得当前运行的job的进度。当多次询问失败后，cli将会访问请求资源管理器，
            看时候app master有了新的位置。（在提交job被调度后，资源管理器会返回job的app master位置给cli）
    3. node manager运行失败
        * 处理策略与MR1相同
    4. 资源管理器失败
        * 其由检查点机制，可在失败后从检查点中恢复
* job的调度策略
    1. 带有优先级的FIFO的非抢占式job调度
    2. 公平调度
        * 每个用户均有其的作业槽，使得每个用户公平共享集群能力
        * 支持抢占式：当一是job池一定时间没有公平共享资源，则会终止得到过多资源的任务
    3. 容量调度
        * 集群资源由多个队列调度，每个队列分配一定的容量调度
